{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-08T19:32:36.547780Z",
     "start_time": "2025-07-08T19:32:31.968534Z"
    }
   },
   "source": [
    "# This notebook uses Chronos's embedding as a feature extractor for Random Forest\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from chronos import ChronosBoltPipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_id = \"amazon/chronos-bolt-base\"\n",
    "\n",
    "pipeline = ChronosBoltPipeline.from_pretrained(\n",
    "    \"amazon/chronos-bolt-base\",\n",
    "    device_map=\"cuda:1\",\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baorenl/chronos/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T19:32:45.258635Z",
     "start_time": "2025-07-08T19:32:45.246881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numpy.fft import fft\n",
    "\n",
    "# HARTH dataset location is ./data/harth\n",
    "\n",
    "class HARTHDataset(Dataset):\n",
    "    def __init__(self, file_names):\n",
    "        self.sessions = []\n",
    "        self.labels = []\n",
    "        self.features = []\n",
    "        chunk_size = 150\n",
    "        step_size = 75\n",
    "\n",
    "        for file_name in file_names:\n",
    "            print(f\"Processing {file_name}\")\n",
    "            raw_path = os.path.join(\"data\", \"harth\", file_name)\n",
    "            df = pd.read_csv(raw_path)\n",
    "\n",
    "            # Filter for the labels 1-8\n",
    "            df_filtered = df[df['label'] <= 8].copy()\n",
    "            # Adjust labels to be 0-indexed for CrossEntropyLoss\n",
    "            df_filtered['label'] = df_filtered['label'] - 1\n",
    "\n",
    "            for i in range(0, len(df_filtered) - chunk_size, step_size):\n",
    "                chunk = df_filtered.iloc[i : i + chunk_size]\n",
    "\n",
    "                # Ensure the chunk contains only one unique activity label\n",
    "                if chunk['label'].nunique() == 1 and len(chunk) == chunk_size:\n",
    "                    back_x = torch.tensor(chunk['back_x'].values, dtype=torch.float32)\n",
    "                    back_y = torch.tensor(chunk['back_y'].values, dtype=torch.float32)\n",
    "                    back_z = torch.tensor(chunk['back_z'].values, dtype=torch.float32)\n",
    "\n",
    "                    # --- Calculate Hand-Crafted Features ---\n",
    "                    mean_back_x, mean_back_y, mean_back_z = back_x.mean(), back_y.mean(), back_z.mean()\n",
    "                    std_back_x, std_back_y, std_back_z = back_x.std(), back_y.std(), back_z.std()\n",
    "                    back_sma = torch.sum(back_x.abs() + back_y.abs() + back_z.abs()) / chunk_size\n",
    "\n",
    "                    # FFT with NumPy\n",
    "                    back_fft_x = torch.tensor(np.abs(fft(back_x.numpy()))[:10], dtype=torch.float32)\n",
    "                    back_fft_y = torch.tensor(np.abs(fft(back_y.numpy()))[:10], dtype=torch.float32)\n",
    "                    back_fft_z = torch.tensor(np.abs(fft(back_z.numpy()))[:10], dtype=torch.float32)\n",
    "\n",
    "                    # Same thing for thigh acc\n",
    "                    thigh_x = torch.tensor(chunk['thigh_x'].values, dtype=torch.float32)\n",
    "                    thigh_y = torch.tensor(chunk['thigh_y'].values, dtype=torch.float32)\n",
    "                    thigh_z = torch.tensor(chunk['thigh_z'].values, dtype=torch.float32)\n",
    "\n",
    "                    mean_thigh_x, mean_thigh_y, mean_thigh_z = thigh_x.mean(), thigh_y.mean(), thigh_z.mean()\n",
    "                    std_thigh_x, std_thigh_y, std_thigh_z = thigh_x.std(), thigh_y.std(), thigh_z.std()\n",
    "                    thigh_sma = torch.sum(thigh_x.abs() + thigh_y.abs() + thigh_z.abs()) / chunk_size\n",
    "\n",
    "                    thigh_fft_x = torch.tensor(np.abs(fft(thigh_x.numpy()))[:10], dtype=torch.float32)\n",
    "                    thigh_fft_y = torch.tensor(np.abs(fft(thigh_y.numpy()))[:10], dtype=torch.float32)\n",
    "                    thigh_fft_z = torch.tensor(np.abs(fft(thigh_z.numpy()))[:10], dtype=torch.float32)\n",
    "\n",
    "                    features = torch.cat([\n",
    "                        torch.tensor([mean_back_x, mean_back_y, mean_back_z]),\n",
    "                        torch.tensor([mean_thigh_x, mean_thigh_y, mean_thigh_z]),\n",
    "                        torch.tensor([std_back_x, std_back_y, std_back_z]),\n",
    "                        torch.tensor([std_thigh_x, std_thigh_y, std_thigh_z]),\n",
    "                        torch.tensor([back_sma]),\n",
    "                        torch.tensor([thigh_sma]),\n",
    "                        back_fft_x,\n",
    "                        back_fft_y,\n",
    "                        back_fft_z,\n",
    "                        thigh_fft_x,\n",
    "                        thigh_fft_y,\n",
    "                        thigh_fft_z,\n",
    "                    ])\n",
    "\n",
    "                    univariate = torch.concat([back_x, back_y, back_z, thigh_x, thigh_y, thigh_z])\n",
    "                    self.sessions.append(univariate)\n",
    "                    self.labels.append(chunk['label'].iloc[0])\n",
    "                    self.features.append(features)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        label = self.labels[idx]\n",
    "        feature = self.features[idx]\n",
    "        return session, feature, label"
   ],
   "id": "7cd5896d69cf5636",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T19:33:28.778927Z",
     "start_time": "2025-07-08T19:32:45.941300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = HARTHDataset([\"S006.csv\", \"S009.csv\", \"S010.csv\", \"S012.csv\", \"S013.csv\", \"S014.csv\", \"S015.csv\", \"S020.csv\", \"S021.csv\", \"S022.csv\", \"S023.csv\", \"S024.csv\", \"S025.csv\", \"S026.csv\", \"S027.csv\", \"S028.csv\", \"S029.csv\"])\n",
    "test_dataset = HARTHDataset([\"S008.csv\", \"S016.csv\", \"S017.csv\", \"S018.csv\", \"S019.csv\"])"
   ],
   "id": "20db30d66349e4f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S006.csv\n",
      "Processing S009.csv\n",
      "Processing S010.csv\n",
      "Processing S012.csv\n",
      "Processing S013.csv\n",
      "Processing S014.csv\n",
      "Processing S015.csv\n",
      "Processing S020.csv\n",
      "Processing S021.csv\n",
      "Processing S022.csv\n",
      "Processing S023.csv\n",
      "Processing S024.csv\n",
      "Processing S025.csv\n",
      "Processing S026.csv\n",
      "Processing S027.csv\n",
      "Processing S028.csv\n",
      "Processing S029.csv\n",
      "Processing S008.csv\n",
      "Processing S016.csv\n",
      "Processing S017.csv\n",
      "Processing S018.csv\n",
      "Processing S019.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T19:33:32.427468Z",
     "start_time": "2025-07-08T19:33:32.423405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)"
   ],
   "id": "bcbbd7b775d788e0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T19:57:29.101689Z",
     "start_time": "2025-07-08T19:57:29.089913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract Features in Batches Using DataLoader\n",
    "def extract_features_from_loader(loader, model_pipeline):\n",
    "    \"\"\"\n",
    "    Iterates through a DataLoader, extracts features for each batch\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    handcrafted_features = []\n",
    "    chronos_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Loop through the data batch by batch\n",
    "    for i, (series_batch, features_batch, labels_batch) in enumerate(loader):\n",
    "        print(f\"  Processing batch {i+1}/{len(loader)}...\")\n",
    "\n",
    "        series_list = [s for s in series_batch]\n",
    "\n",
    "        embeddings_batch, _ = model_pipeline.embed(series_list)\n",
    "\n",
    "        chronos_features = embeddings_batch.mean(dim=1).numpy()\n",
    "        combined_features = np.hstack([chronos_features, features_batch.numpy()])\n",
    "\n",
    "        all_features.append(combined_features)\n",
    "        handcrafted_features.append(features_batch.numpy())\n",
    "        chronos_embeddings.append(chronos_features)\n",
    "        all_labels.append(labels_batch.numpy())\n",
    "\n",
    "    # returns chronos embeddings, handcrafted features, all features, and all labels\n",
    "    return np.vstack(chronos_embeddings), np.vstack(handcrafted_features), np.vstack(all_features), np.concatenate(all_labels)"
   ],
   "id": "eb0dbb59104e03af",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T19:59:21.030643Z",
     "start_time": "2025-07-08T19:57:29.664546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_chronos, x_train_hc, x_train_combined, y_train = extract_features_from_loader(train_loader, pipeline)\n",
    "x_test_chronos, x_test_hc, x_test_combined, y_test = extract_features_from_loader(test_loader, pipeline)"
   ],
   "id": "aeb9227f9e1ba6e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 1/13...\n",
      "  Processing batch 2/13...\n",
      "  Processing batch 3/13...\n",
      "  Processing batch 4/13...\n",
      "  Processing batch 5/13...\n",
      "  Processing batch 6/13...\n",
      "  Processing batch 7/13...\n",
      "  Processing batch 8/13...\n",
      "  Processing batch 9/13...\n",
      "  Processing batch 10/13...\n",
      "  Processing batch 11/13...\n",
      "  Processing batch 12/13...\n",
      "  Processing batch 13/13...\n",
      "  Processing batch 1/5...\n",
      "  Processing batch 2/5...\n",
      "  Processing batch 3/5...\n",
      "  Processing batch 4/5...\n",
      "  Processing batch 5/5...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RF on Chronos embedding",
   "id": "4ebfddbbe9ec4030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T23:29:00.041552Z",
     "start_time": "2025-07-08T23:28:52.629962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=150, n_jobs=-1, class_weight='balanced')\n",
    "classifier.fit(x_train_chronos, y_train)\n",
    "y_pred = classifier.predict(x_test_chronos)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ],
   "id": "b366fb7b5cce4e0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9683051435940677\n",
      "[[ 1849     3    18     3     0    28     3     0]\n",
      " [   27   285     0     0     0     0     0     0]\n",
      " [   90     0    65     0     0    92    10     0]\n",
      " [  121     0     1    16     0     3     2     0]\n",
      " [   77     0     0     0     7     1     0     0]\n",
      " [   24     0    26     0     0  1845    25     0]\n",
      " [   21     0     0     0     0    18 13681     1]\n",
      " [   13     0     0     0     0     8    24  1774]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RF on hand crafted features",
   "id": "98dfaa67fcc733ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T23:30:01.102769Z",
     "start_time": "2025-07-08T23:29:59.491317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier.fit(x_train_hc, y_train)\n",
    "y_pred = classifier.predict(x_test_hc)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ],
   "id": "941c9dccce641876",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9713803878775854\n",
      "[[ 1841    11    31     5     2    14     0     0]\n",
      " [   11   300     0     0     1     0     0     0]\n",
      " [   79     0    88     0     0    83     7     0]\n",
      " [   93     0     1    46     0     3     0     0]\n",
      " [   59     0     0     0    26     0     0     0]\n",
      " [   30     0    45     0     0  1844     1     0]\n",
      " [   20     0     0     0     0     0 13700     1]\n",
      " [    0     0     0     0     0     0    80  1739]]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RF on combined features",
   "id": "86b9c680f531b384"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T23:30:13.943279Z",
     "start_time": "2025-07-08T23:30:07.935249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier.fit(x_train_combined, y_train)\n",
    "y_pred = classifier.predict(x_test_combined)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ],
   "id": "4cc719a027cec567",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9723228014483408\n",
      "[[ 1844     7    20     3     0    27     3     0]\n",
      " [   18   294     0     0     0     0     0     0]\n",
      " [   84     0    73     0     0    93     7     0]\n",
      " [  118     0     0    21     0     3     1     0]\n",
      " [   74     0     0     0    10     1     0     0]\n",
      " [   28     0    30     0     0  1862     0     0]\n",
      " [   21     0     0     0     0     1 13699     0]\n",
      " [    5     0     0     0     0     0    14  1800]]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3dfbe62db7b5af36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
